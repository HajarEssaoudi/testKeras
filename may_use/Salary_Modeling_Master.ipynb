{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Modeling\" data-toc-modified-id=\"Modeling-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Modeling</a></span><ul class=\"toc-item\"><li><span><a href=\"#Import-Libraries-and-the-final-salary-data-set\" data-toc-modified-id=\"Import-Libraries-and-the-final-salary-data-set-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Import Libraries and the final salary data set</a></span><ul class=\"toc-item\"><li><span><a href=\"#Set-up-data-and-target\" data-toc-modified-id=\"Set-up-data-and-target-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>Set up data and target</a></span><ul class=\"toc-item\"><li><span><a href=\"#Train-Test-Split\" data-toc-modified-id=\"Train-Test-Split-1.1.1.1\"><span class=\"toc-item-num\">1.1.1.1&nbsp;&nbsp;</span>Train-Test Split</a></span></li></ul></li></ul></li><li><span><a href=\"#Model-1:--Use-Word2Vec-Embedding-in-Classification-Models\" data-toc-modified-id=\"Model-1:--Use-Word2Vec-Embedding-in-Classification-Models-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Model 1:  Use Word2Vec Embedding in Classification Models</a></span><ul class=\"toc-item\"><li><span><a href=\"#Word2Vec-Model-from-scratch\" data-toc-modified-id=\"Word2Vec-Model-from-scratch-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>Word2Vec Model from scratch</a></span></li><li><span><a href=\"#Use-Word2Vec-in-the-pipeline\" data-toc-modified-id=\"Use-Word2Vec-in-the-pipeline-1.2.2\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span>Use Word2Vec in the pipeline</a></span></li><li><span><a href=\"#Prediction-Probability-of-each-bin\" data-toc-modified-id=\"Prediction-Probability-of-each-bin-1.2.3\"><span class=\"toc-item-num\">1.2.3&nbsp;&nbsp;</span>Prediction Probability of each bin</a></span></li><li><span><a href=\"#Random-Forest-with-Grid-Search\" data-toc-modified-id=\"Random-Forest-with-Grid-Search-1.2.4\"><span class=\"toc-item-num\">1.2.4&nbsp;&nbsp;</span>Random Forest with Grid Search</a></span></li><li><span><a href=\"#SGD-with-Grid-Search\" data-toc-modified-id=\"SGD-with-Grid-Search-1.2.5\"><span class=\"toc-item-num\">1.2.5&nbsp;&nbsp;</span>SGD with Grid Search</a></span></li></ul></li><li><span><a href=\"#Model-2:-Count-vectorizer-+-TF-IDF-with-GridSearch\" data-toc-modified-id=\"Model-2:-Count-vectorizer-+-TF-IDF-with-GridSearch-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Model 2: Count vectorizer + TF-IDF with GridSearch</a></span><ul class=\"toc-item\"><li><span><a href=\"#Train-Test-Split\" data-toc-modified-id=\"Train-Test-Split-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>Train-Test Split</a></span></li><li><span><a href=\"#SGD-Classifier-with-Grid-Search\" data-toc-modified-id=\"SGD-Classifier-with-Grid-Search-1.3.2\"><span class=\"toc-item-num\">1.3.2&nbsp;&nbsp;</span>SGD Classifier with Grid Search</a></span><ul class=\"toc-item\"><li><span><a href=\"#Confusion-Matrix\" data-toc-modified-id=\"Confusion-Matrix-1.3.2.1\"><span class=\"toc-item-num\">1.3.2.1&nbsp;&nbsp;</span>Confusion Matrix</a></span></li></ul></li><li><span><a href=\"#Random-Forest-Classifier-with-Grid-Search\" data-toc-modified-id=\"Random-Forest-Classifier-with-Grid-Search-1.3.3\"><span class=\"toc-item-num\">1.3.3&nbsp;&nbsp;</span>Random Forest Classifier with Grid Search</a></span></li></ul></li><li><span><a href=\"#Model-3:-Use-Pretrained-GloVe-Model\" data-toc-modified-id=\"Model-3:-Use-Pretrained-GloVe-Model-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Model 3: Use Pretrained GloVe Model</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Getting-the-Total-Vocabulary\" data-toc-modified-id=\"Getting-the-Total-Vocabulary-1.4.0.1\"><span class=\"toc-item-num\">1.4.0.1&nbsp;&nbsp;</span>Getting the Total Vocabulary</a></span></li></ul></li><li><span><a href=\"#Random-Forest-with-Grid-Search\" data-toc-modified-id=\"Random-Forest-with-Grid-Search-1.4.1\"><span class=\"toc-item-num\">1.4.1&nbsp;&nbsp;</span>Random Forest with Grid Search</a></span></li><li><span><a href=\"#SGD-Classifier-in-the-pipeline\" data-toc-modified-id=\"SGD-Classifier-in-the-pipeline-1.4.2\"><span class=\"toc-item-num\">1.4.2&nbsp;&nbsp;</span>SGD Classifier in the pipeline</a></span></li></ul></li><li><span><a href=\"#Model-4:-Deep-Learning-Models-with-Word-Embeddings\" data-toc-modified-id=\"Model-4:-Deep-Learning-Models-with-Word-Embeddings-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Model 4: Deep Learning Models with Word Embeddings</a></span><ul class=\"toc-item\"><li><span><a href=\"#Word-Embedding\" data-toc-modified-id=\"Word-Embedding-1.5.1\"><span class=\"toc-item-num\">1.5.1&nbsp;&nbsp;</span>Word Embedding</a></span><ul class=\"toc-item\"><li><span><a href=\"#Train-Test-Split\" data-toc-modified-id=\"Train-Test-Split-1.5.1.1\"><span class=\"toc-item-num\">1.5.1.1&nbsp;&nbsp;</span>Train-Test Split</a></span></li></ul></li><li><span><a href=\"#Create-a-base-model\" data-toc-modified-id=\"Create-a-base-model-1.5.2\"><span class=\"toc-item-num\">1.5.2&nbsp;&nbsp;</span>Create a base model</a></span></li><li><span><a href=\"#Hyperparameter-tuning-on-a-regular-embedding-layer\" data-toc-modified-id=\"Hyperparameter-tuning-on-a-regular-embedding-layer-1.5.3\"><span class=\"toc-item-num\">1.5.3&nbsp;&nbsp;</span>Hyperparameter tuning on a regular embedding layer</a></span></li></ul></li><li><span><a href=\"#Model-5:-Use-pre-trained-Word2Vec-as-the-embedding-layer\" data-toc-modified-id=\"Model-5:-Use-pre-trained-Word2Vec-as-the-embedding-layer-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Model 5: Use pre-trained Word2Vec as the embedding layer</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Tuning-the-model\" data-toc-modified-id=\"Tuning-the-model-1.6.0.1\"><span class=\"toc-item-num\">1.6.0.1&nbsp;&nbsp;</span>Tuning the model</a></span></li><li><span><a href=\"#Grid-Search\" data-toc-modified-id=\"Grid-Search-1.6.0.2\"><span class=\"toc-item-num\">1.6.0.2&nbsp;&nbsp;</span>Grid Search</a></span></li></ul></li><li><span><a href=\"#Try-Count-Vectorizer-and-TFIDF-in-Deep-Learning-Models-with-Grid-Search\" data-toc-modified-id=\"Try-Count-Vectorizer-and-TFIDF-in-Deep-Learning-Models-with-Grid-Search-1.6.1\"><span class=\"toc-item-num\">1.6.1&nbsp;&nbsp;</span>Try Count Vectorizer and TFIDF in Deep Learning Models with Grid Search</a></span><ul class=\"toc-item\"><li><span><a href=\"#Grid-Search\" data-toc-modified-id=\"Grid-Search-1.6.1.1\"><span class=\"toc-item-num\">1.6.1.1&nbsp;&nbsp;</span>Grid Search</a></span></li></ul></li></ul></li><li><span><a href=\"#Model-6:-Mixed-Input-Model-with-City,-Rate-Type-and-Cost-of-Living-Indexes\" data-toc-modified-id=\"Model-6:-Mixed-Input-Model-with-City,-Rate-Type-and-Cost-of-Living-Indexes-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>Model 6: Mixed Input Model with City, Rate Type and Cost of Living Indexes</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Explore-continuous-variables-and-categorical-variables\" data-toc-modified-id=\"Explore-continuous-variables-and-categorical-variables-1.7.0.1\"><span class=\"toc-item-num\">1.7.0.1&nbsp;&nbsp;</span>Explore continuous variables and categorical variables</a></span></li></ul></li><li><span><a href=\"#Put-them-together\" data-toc-modified-id=\"Put-them-together-1.7.1\"><span class=\"toc-item-num\">1.7.1&nbsp;&nbsp;</span>Put them together</a></span></li><li><span><a href=\"#Try-Word2Vec-Embedding-in-Mixed-Input-Model\" data-toc-modified-id=\"Try-Word2Vec-Embedding-in-Mixed-Input-Model-1.7.2\"><span class=\"toc-item-num\">1.7.2&nbsp;&nbsp;</span>Try Word2Vec Embedding in Mixed Input Model</a></span></li></ul></li><li><span><a href=\"#Model-7:-Use-Job-Title-along-with-City,-Cost-of-Living-Index-in-Classification-Models\" data-toc-modified-id=\"Model-7:-Use-Job-Title-along-with-City,-Cost-of-Living-Index-in-Classification-Models-1.8\"><span class=\"toc-item-num\">1.8&nbsp;&nbsp;</span>Model 7: Use Job Title along with City, Cost of Living Index in Classification Models</a></span><ul class=\"toc-item\"><li><span><a href=\"#Explore-Title\" data-toc-modified-id=\"Explore-Title-1.8.1\"><span class=\"toc-item-num\">1.8.1&nbsp;&nbsp;</span>Explore Title</a></span></li><li><span><a href=\"#Random-Forest-as-base-model\" data-toc-modified-id=\"Random-Forest-as-base-model-1.8.2\"><span class=\"toc-item-num\">1.8.2&nbsp;&nbsp;</span>Random Forest as base model</a></span></li><li><span><a href=\"#XGBoost\" data-toc-modified-id=\"XGBoost-1.8.3\"><span class=\"toc-item-num\">1.8.3&nbsp;&nbsp;</span>XGBoost</a></span></li></ul></li></ul></li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Conclusion</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LPrg-wLJde1r"
   },
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dkTX4ZxFde1s"
   },
   "source": [
    "## Import Libraries and the final salary data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KgjYcXBGde1t"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#NLP\n",
    "from nltk import word_tokenize, FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.collocations import *\n",
    "import string, re\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#Model Data Prep\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "#Machine Learning\n",
    "from sklearn.metrics import accuracy_score, f1_score, mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import LinearRegression, SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#Deep learning\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Dense, LSTM, Embedding\n",
    "from keras.layers import Dropout, Activation, Bidirectional, GlobalMaxPool1D\n",
    "from keras.layers.convolutional import Conv1D, Conv2D\n",
    "from keras.layers.convolutional import MaxPooling1D, MaxPooling2D\n",
    "from keras.models import Sequential\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.layers import Flatten\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import regularizers\n",
    "from keras.wrappers.scikit_learn import KerasClassifier #to use Keras in sklearn\n",
    "\n",
    "#Deep Learning - Mixed inputs\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras.layers import concatenate\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "\n",
    "#Save Model\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "import logging\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 59741,
     "status": "ok",
     "timestamp": 1590874287738,
     "user": {
      "displayName": "Bonnie Ma",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj0duL5aTKrqBP362PgHUpwQ_3yTA5e2b6MsV-F=s64",
      "userId": "16062141668243618164"
     },
     "user_tz": 240
    },
    "id": "-RDL6eIhde1z",
    "outputId": "2a0b36ca-82b0-49c9-8be4-f873db17024f"
   },
   "outputs": [],
   "source": [
    "salary_df = pd.read_csv(\"salary_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 164
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 516,
     "status": "error",
     "timestamp": 1590873846701,
     "user": {
      "displayName": "Bonnie Ma",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj0duL5aTKrqBP362PgHUpwQ_3yTA5e2b6MsV-F=s64",
      "userId": "16062141668243618164"
     },
     "user_tz": 240
    },
    "id": "f1wMqzFmde12",
    "outputId": "b7f2da49-6002-4e28-df06-e7968d5057ad"
   },
   "outputs": [],
   "source": [
    "salary_df = salary_df.drop_duplicates(subset=\"Link\", keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EDxA_PQ7de17",
    "outputId": "73bd1b11-06c0-4c18-a2f1-a7548002b7b3"
   },
   "outputs": [],
   "source": [
    "salary_df.hist('adjusted_yearly_salary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m5sKPx_Ide1_",
    "outputId": "611794c0-0337-4cd6-bdde-c977c4909b18"
   },
   "outputs": [],
   "source": [
    "salary_df['adjusted_yearly_salary'].max() - salary_df['adjusted_yearly_salary'].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JZbWB7hbde2C"
   },
   "source": [
    "Salary bins: [< 50,000, 50,000-75,000, 75,000-100,000, 100,000-125,000, 125,000-150,000, 150,000-175,000, 175,000-200,000, 200,000-225,000, 225,000-250,000, >250,000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hd-3IVznde2D",
    "outputId": "1b484a63-d4fb-43d0-baa5-c06f4456e0ed"
   },
   "outputs": [],
   "source": [
    "salary_df['adjusted_yearly_salary'].quantile(list(np.linspace(0,1,7)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aVZgrcgvde2G"
   },
   "outputs": [],
   "source": [
    "#Create Bins\n",
    "BINS = [0,50000,70000,90000,120000,150000,400000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G5DgQQK_de2J"
   },
   "outputs": [],
   "source": [
    "salary_df['Salary_Bins'] = pd.cut(x=salary_df['adjusted_yearly_salary'], bins=BINS, labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R3nP4LoVde2M",
    "outputId": "51fbccda-f6c1-4745-9215-7612f2d783ac"
   },
   "outputs": [],
   "source": [
    "salary_df['Salary_Bins']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_df.to_csv(\"salary_final_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rXB4mFKPde2P",
    "outputId": "171cbcc6-7848-486f-ff92-65ee4c2d162a"
   },
   "outputs": [],
   "source": [
    "salary_df.groupby('Salary_Bins')['Location2'].count().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8mQWqLklde2R"
   },
   "outputs": [],
   "source": [
    "#Categorical target\n",
    "target_class = salary_df['Salary_Bins'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_word(job_descriptions):\n",
    "    jd_data=[]\n",
    "    pattern = \"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    stopwords_list += list(string.punctuation)\n",
    "    stopwords_list += ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "    stopwords_list += ['www','http','com','llc','covid','non','pm','am','eg','e','g','ie','i']\n",
    "    \n",
    "    for jd in job_descriptions:\n",
    "        jd_tokens_raw = nltk.regexp_tokenize(jd, pattern)\n",
    "        jd_tokens=[word.lower() for word in jd_tokens_raw]\n",
    "        jd_words_stopped = [word for word in jd_tokens if word not in stopwords_list]\n",
    "        jd_data.append(jd_words_stopped)\n",
    "\n",
    "    return pd.Series(jd_data) # Turn list of lists to series of lists to use in Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = clean_word(salary_df['combined_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up data and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d3CTDfDQde2W"
   },
   "outputs": [],
   "source": [
    "data = clean_word(salary_df['combined_text'])\n",
    "#target = salary_df['adjusted_yearly_salary'] #continuous target\n",
    "target_class = salary_df['Salary_Bins'].astype('category') #categorical target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train-Test Split\n",
    "\n",
    "data is tokenized job description, target_class is salary classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, target_class, test_size=0.1, random_state=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ulhtwoNAde2Z"
   },
   "source": [
    "## Model 1:  Use Word2Vec Embedding in Classification Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FxXpOkfxde2Z"
   },
   "source": [
    "We are going to use Word2Vec model in this section. Word2vec is a group of related models that are used to produce word embeddings. These models are shallow, two-layer neural networks that are trained to reconstruct linguistic contexts of words. Word2vec takes as its input a large corpus of text and produces a vector space, typically of several hundred dimensions, with each unique word in the corpus being assigned a corresponding vector in the space. Word vectors are positioned in the vector space such that words that share common contexts in the corpus are located close to one another in the space.\n",
    "\n",
    "We need to specify following parameter to the model:\n",
    "* the `size` of the word vectors we want to learn \n",
    "* the `window` size to use when training the model\n",
    "* `min_count`, which corresponds to the minimum number of times a word must be used in the corpus in order to be included in the training (for instance, `min_count=5` would only learn word embeddings for words that appear 5 or more times throughout the entire training set)\n",
    "* `workers`, the number of threads to use for training, which can speed up processing (`4` is typically used, since most processors nowadays have at least 4 cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lNyuoEtbde2a",
    "outputId": "9345c09a-c451-4c32-a380-fc5ed50dafed"
   },
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3IHAm2Bode2d"
   },
   "source": [
    "http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/\n",
    "TF-IDF embedding, focus on important words.\n",
    "<br>https://scikit-learn.org/stable/modules/classes.html<br>\n",
    "\n",
    "#module-sklearn.feature_extraction.text\n",
    "<br>https://scikit-learn.org/stable/auto_examples/model_selection/grid_search_text_feature_extraction.html<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cqMsrOjGde2d"
   },
   "source": [
    "### Word2Vec Model from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Word2Vec model with 100 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nw7rYGqdde2e"
   },
   "outputs": [],
   "source": [
    "w2v_model1 = Word2Vec(data,\n",
    "                size=64, #size is the number of dimensions of the N-dimensional space and Word2Vec maps the words into\n",
    "                window=10,\n",
    "                min_count=10, #min frequency appeared in the corpus\n",
    "                workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UZ0XpKJkde2h",
    "outputId": "06eb2c99-3434-4668-d890-e61ea841fbb3"
   },
   "outputs": [],
   "source": [
    "w2v_model1.train(data,total_examples=w2v_model1.corpus_count, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "60ZVS0yRde2o"
   },
   "outputs": [],
   "source": [
    "wv = w2v_model1.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5qIEslLmde2r",
    "outputId": "7f6a50ea-bf60-4759-ab32-053e6572bfed"
   },
   "outputs": [],
   "source": [
    "#After filtering for min frequency, there are 5514 words in the corpus\n",
    "words = list(wv.vocab)\n",
    "print('Vocabulary size:%d' % len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "71xJIPOqde2t",
    "outputId": "71cd73e4-5d2f-410c-bcca-3a4ff093e87c",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "wv.most_similar('data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aTdsdQNxde2v"
   },
   "source": [
    "To be able to use in a pipeline, need fit and transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QBpVqOQnde2v",
    "outputId": "a0718a80-fc40-4838-b0fe-b953f590cd63"
   },
   "outputs": [],
   "source": [
    "#wrap into a dictionary to use in the pipeline\n",
    "w2v = dict(zip(w2v_model1.wv.index2word, w2v_model1.wv.vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BGgMRpmxde2y",
    "outputId": "393b5dc6-2d7a-445a-a966-61ff23fd6301",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(w2v.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SbWOxcOade20"
   },
   "source": [
    "Averaging word vectors for all words in a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DULSNPEFde21"
   },
   "outputs": [],
   "source": [
    "#Averaging word vectors for all words in a text.\n",
    "\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = len(word2vec.values())\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Word2Vec in the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "import logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a8e477ezde23"
   },
   "outputs": [],
   "source": [
    "#Use pipeline \n",
    "\n",
    "rf =  Pipeline([('Word2Vec Vectorizer', MeanEmbeddingVectorizer(w2v)),\n",
    "              ('Random Forest', RandomForestClassifier(n_estimators=50, criterion='entropy', verbose=True, n_jobs=3))])\n",
    "\n",
    "svc = Pipeline([('Word2Vec Vectorizer', MeanEmbeddingVectorizer(w2v)),\n",
    "              ('Support Vector Machine', SVC(kernel='rbf', C=0.5))])\n",
    "\n",
    "sgd = Pipeline([('Word2Vec Vectorizer', MeanEmbeddingVectorizer(w2v)),\n",
    "               ('Stochastic Gradient Descent', SGDClassifier(alpha=0.001, n_jobs=3))])\n",
    "\n",
    "#svr = Pipeline([('Word2Vec Vectorizer', MeanEmbeddingVectorizer(w2v)),\n",
    "              #('SVR-lin', SVR(kernel='rbf', C=0.5))])\n",
    "                \n",
    "#lr = Pipeline([('Word2Vec Vectorizer', MeanEmbeddingVectorizer(w2v)),\n",
    "              #('Linear Regression', LinearRegression(n_jobs=3))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1oM8V_MRde3A"
   },
   "outputs": [],
   "source": [
    "models = [('Random Forest', rf),\n",
    "          ('Support Vector Machine', svc),\n",
    "          ('Stochastic Gradient Descent', sgd)]\n",
    "          #('Linear Regression', lr)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bqilR_tLde3C",
    "outputId": "28c64516-689b-4e27-8444-e208a04b41ff"
   },
   "outputs": [],
   "source": [
    "scores = [(name, cross_val_score(model, X_train, y_train, cv=2).mean()) for name, model, in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y-FaU-O1de3E",
    "outputId": "3fec4d8f-517e-4460-f386-30f8592df0d2"
   },
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest performed the best among three models. Explore the prediction probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YbxuzGm8de25"
   },
   "outputs": [],
   "source": [
    "rf =  Pipeline([('Word2Vec Vectorizer', MeanEmbeddingVectorizer(w2v)),\n",
    "              ('Random Forest', RandomForestClassifier(n_estimators=100, criterion='entropy', min_samples_leaf=3, verbose=True, n_jobs=3))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I074RlvRde29",
    "outputId": "771787cc-a8e9-4dc6-ec85-dfa7051890a0"
   },
   "outputs": [],
   "source": [
    "cross_val_score(rf, X_train, y_train, cv=2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba_matrix = rf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Probability of each bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_proba = pd.DataFrame(data = proba_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_proba.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_IoNvOQ6de3G"
   },
   "source": [
    "We got a test accuracy of 0.48 with a simple RandomForest model. Worthy to do some grid search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest with Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AQACO63Mde3I"
   },
   "outputs": [],
   "source": [
    "#Grid Search: Word2Vec with Random Forest Classifier\n",
    "pipeline =  Pipeline([('Word2Vec Vectorizer', MeanEmbeddingVectorizer(w2v)),\n",
    "              ('rf', RandomForestClassifier(n_jobs=3))])\n",
    "parameters = {\n",
    "    'rf__n_estimators':(30,40,50),\n",
    "    'rf__criterion':(\"gini\",\"entropy\"),\n",
    "    'rf__min_samples_split':(2,6,8),\n",
    "    'rf__max_features':(\"auto\",\"sqrt\",\"log2\")\n",
    "    \n",
    "}\n",
    "                \n",
    "if __name__ == \"__main__\":\n",
    "    # multiprocessing requires the fork to happen in a __main__ protected\n",
    "    # block\n",
    "\n",
    "    # find the best parameters for both the feature extraction and the\n",
    "    # classifier\n",
    "    grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)\n",
    "\n",
    "    print(\"Performing grid search...\")\n",
    "    print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "    print(\"parameters:\")\n",
    "    pprint(parameters)\n",
    "    t0 = time()\n",
    "    grid_search.fit(data, target_class)\n",
    "    print(\"done in %0.3fs\" % (time() - t0))\n",
    "    print()\n",
    "\n",
    "    print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_8woTy_5de3L"
   },
   "source": [
    "Model performance was improved a little bit. Try another grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yFyq9_2Cde3N"
   },
   "outputs": [],
   "source": [
    "#Grid Search: Word2Vec with Random Forest Classifier\n",
    "pipeline =  Pipeline([('Word2Vec Vectorizer', MeanEmbeddingVectorizer(w2v)),\n",
    "              ('rf', RandomForestClassifier(n_estimators = 30, criterion='gini', n_jobs=3, max_features='sqrt'))])\n",
    "parameters = {\n",
    "    'rf__class_weight':(\"balanced\",None),\n",
    "    'rf__min_samples_split':(4,6),\n",
    "    'rf__max_samples':(None, 0.5,0.8,1)   \n",
    "}\n",
    "                \n",
    "if __name__ == \"__main__\":\n",
    "    # multiprocessing requires the fork to happen in a __main__ protected\n",
    "    # block\n",
    "\n",
    "    # find the best parameters for both the feature extraction and the\n",
    "    # classifier\n",
    "    grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)\n",
    "\n",
    "    print(\"Performing grid search...\")\n",
    "    print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "    print(\"parameters:\")\n",
    "    pprint(parameters)\n",
    "    t0 = time()\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    print(\"done in %0.3fs\" % (time() - t0))\n",
    "    print()\n",
    "\n",
    "    print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results improved from 0.40 to 0.46. Best estimator set so far: \n",
    "Best parameters set:\n",
    "\n",
    "\t- rf__class_weight: None\n",
    "\t- rf__max_samples: 0.8\n",
    "\t- rf__min_samples_split: 4\n",
    "    - rf__criterion: 'gini'\n",
    "\t- rf__max_features: 'sqrt'\n",
    "\t- rf__n_estimators: 30\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used Word2Vec model to vectorize job descriptions and then use word vectors as inputs (100 dimensions) in the Random Forest model. Next we will try sklearn build in Count Vectorizer and then transform a count matrix to a normalized TF-IDF representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  SGD with Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try SGDClassifier \n",
    "pipeline = Pipeline([\n",
    "    ('Word2Vec Vectorizer', MeanEmbeddingVectorizer(w2v)),\n",
    "    ('clf', SGDClassifier(n_jobs=3, early_stopping=True, validation_fraction=0.2))\n",
    "])\n",
    "\n",
    "# uncommenting more parameters will give better exploring power but will\n",
    "# increase processing time in a combinatorial way\n",
    "parameters = {\n",
    "    \n",
    "    'clf__alpha': (0.0001, 0.001, 0.01),\n",
    "    'clf__penalty': ('l2', 'elasticnet'),\n",
    "    'clf__l1_ratio':(0, 0.1, 0.3, 0.5),\n",
    "    'clf__max_iter': (100,200,300)\n",
    "}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # multiprocessing requires the fork to happen in a __main__ protected\n",
    "    # block\n",
    "\n",
    "    # find the best parameters for both the feature extraction and the\n",
    "    # classifier\n",
    "    grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)\n",
    "\n",
    "    print(\"Performing grid search...\")\n",
    "    print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "    print(\"parameters:\")\n",
    "    pprint(parameters)\n",
    "    t0 = time()\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    print(\"done in %0.3fs\" % (time() - t0))\n",
    "    print()\n",
    "\n",
    "    print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGD did not perform very well even after Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: Count vectorizer + TF-IDF with GridSearch\n",
    "\n",
    "Tf means term-frequency while tf-idf means term-frequency times inverse document-frequency. This is a common term weighting scheme in information retrieval. The goal of using tf-idf instead of the raw frequencies of occurence of a token in a given document is to scale down the impact of tokens that occur very frequently in a given corpus and are hence empirically less informative than features that occur in a small fraction of the training corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from time import time\n",
    "import logging\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The input needs to be raw text\n",
    "text = salary_df['combined_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(text, target_class, test_size=0.1, random_state=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD Classifier with Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try SGDClassifier \n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(max_df=0.5, min_df = 5, ngram_range=(1,2), stop_words='english')),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SGDClassifier(n_jobs=3))\n",
    "])\n",
    "\n",
    "# uncommenting more parameters will give better exploring power but will\n",
    "# increase processing time in a combinatorial way\n",
    "parameters = {\n",
    "    'vect__max_df':(0.2,0.5),\n",
    "    'vect__min_df':(0.01,0.05),\n",
    "    #'tfidf__use_idf':(True, False),\n",
    "    #'tfidf__smooth_idf':(True, False),\n",
    "    'clf__alpha': (0.0001,0.001),\n",
    "    'clf__loss': ('hinge', 'log'), \n",
    "    #'clf__penalty': ('l2', 'elasticnet'),\n",
    "    'clf__max_iter': (100,200,500)\n",
    "}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # multiprocessing requires the fork to happen in a __main__ protected\n",
    "    # block\n",
    "\n",
    "    # find the best parameters for both the feature extraction and the\n",
    "    # classifier\n",
    "    grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)\n",
    "\n",
    "    print(\"Performing grid search...\")\n",
    "    print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "    print(\"parameters:\")\n",
    "    pprint(parameters)\n",
    "    t0 = time()\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    print(\"done in %0.3fs\" % (time() - t0))\n",
    "    print()\n",
    "\n",
    "    print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SGD = Pipeline([\n",
    "    ('vect', CountVectorizer(max_df=0.5, min_df=0.01, ngram_range=(1,3), stop_words='english')),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SGDClassifier(alpha=0.001, max_iter=500, n_jobs=3))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SGD.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = SGD.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize Confusion Matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "class_names = [\"0-50K\",\"50K-70K\",\"70K-90K\",\"90K-120K\",'120K-150K','150K and above']\n",
    "# Plot non-normalized confusion matrix\n",
    "titles_options = [(\"Confusion matrix, without normalization\", None),\n",
    "                  (\"Normalized confusion matrix\", 'true')]\n",
    "\n",
    "\n",
    "for title, normalize in titles_options:\n",
    "    \n",
    "    disp = plot_confusion_matrix(classifier, X_test, y_test,\n",
    "                                 display_labels=class_names,\n",
    "                                 cmap=plt.cm.Blues,\n",
    "                                 normalize=normalize)\n",
    "    disp.ax_.set_title(title)\n",
    "    \n",
    "    print(title)\n",
    "    print(disp.confusion_matrix)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mean accuracy on the given test data and labels\n",
    "SGD.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model to disk\n",
    "filename='finalized_model.sav'\n",
    "pickle.dump(SGD, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the model from disk\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "result = loaded_model.score(X_test, y_test)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Count vectorizer and TFIDF transformer with SGD did much better than previous Word2Vec and RandomForest. Looks  like TFIDF was able to retrieve valuable information. \n",
    "\n",
    "**Parameters**\n",
    "SGD = Pipeline([\n",
    "    ('vect', CountVectorizer(max_df=0.5, min_df=0.01, ngram_range=(1,3), stop_words='english')),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SGDClassifier(alpha=0.0001, max_iter=500, n_jobs=3))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier with Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(max_df=0.5, min_df=0.01, ngram_range=(1,2), stop_words='english')),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('rf', RandomForestClassifier(n_jobs=3))\n",
    "])\n",
    "\n",
    "# uncommenting more parameters will give better exploring power but will\n",
    "# increase processing time in a combinatorial way\n",
    "parameters = {\n",
    "    \n",
    "    'tfidf__use_idf':(True, False),\n",
    "    'tfidf__smooth_idf':(True, False),\n",
    "    'rf__n_estimators':(30,40,50),\n",
    "    'rf__criterion':(\"gini\",\"entropy\"),\n",
    "    'rf__min_samples_split':(2,6,8),\n",
    "    'rf__max_features':(\"auto\",\"sqrt\",\"log2\")\n",
    "    \n",
    "}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # multiprocessing requires the fork to happen in a __main__ protected\n",
    "    # block\n",
    "\n",
    "    # find the best parameters for both the feature extraction and the\n",
    "    # classifier\n",
    "    grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)\n",
    "\n",
    "    print(\"Performing grid search...\")\n",
    "    print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "    print(\"parameters:\")\n",
    "    pprint(parameters)\n",
    "    t0 = time()\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    print(\"done in %0.3fs\" % (time() - t0))\n",
    "    print()\n",
    "\n",
    "    print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance similar to SGD. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = Pipeline([\n",
    "    ('vect', CountVectorizer(max_df=0.5, min_df=0.03, ngram_range=(1,3), stop_words='english')),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('rf', RandomForestClassifier(max_features='sqrt', min_samples_split=8, n_estimators=50, n_jobs=3))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGD outperforms RandomForest in this case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5lqFFadrde31"
   },
   "source": [
    "## Model 3: Use Pretrained GloVe Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xQaSjclkde32"
   },
   "source": [
    "#### Getting the Total Vocabulary\n",
    "\n",
    "Sort vocab by frequency, take top 200, set that = sub set. Use that to get vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vUF42ZNzde32"
   },
   "outputs": [],
   "source": [
    "total_vocabulary = set(word for jd in data for word in jd)\n",
    "\n",
    "#get glove for the limit_vocabulary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YSDuTyA0de35",
    "outputId": "865fbce3-dc5d-4747-ebf3-e24bf182361b"
   },
   "outputs": [],
   "source": [
    "max_length=len(total_vocabulary)\n",
    "print('There are {} unique tokens in the dataset.'.format(len(total_vocabulary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "okhX8DYAde37"
   },
   "outputs": [],
   "source": [
    "#Get the appropriate vectors out of the GloVe file\n",
    "glove = {}\n",
    "with open('glove_6B_50d.txt', 'rb') as f:\n",
    "    for line in f:\n",
    "        parts = line.split()\n",
    "        word = parts[0].decode('utf-8')\n",
    "        if word in total_vocabulary:   \n",
    "            vector = np.array(parts[1:], dtype=np.float32)\n",
    "            glove[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1wK-2uUdde3-",
    "outputId": "f816c5de-66d5-4527-eb6d-e5d7467ab3dd"
   },
   "outputs": [],
   "source": [
    "#Each word has a 50 dimension vector space\n",
    "glove['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zpGfK25hde4B"
   },
   "outputs": [],
   "source": [
    "len(next(iter(glove)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Oedt1e-Zde4G"
   },
   "outputs": [],
   "source": [
    "class W2vVectorizer(object):\n",
    "    \n",
    "    def __init__(self, w2v):\n",
    "        # Takes in a dictionary of words and vectors as input\n",
    "        self.w2v = w2v\n",
    "        if len(w2v) == 0:\n",
    "            self.dimensions = 0\n",
    "        else:\n",
    "            self.dimensions = len(w2v[next(iter(glove))])\n",
    "    \n",
    "    # Note: Even though it doesn't do anything, it's required that this object implement a fit method or else\n",
    "    # it can't be used in a scikit-learn pipeline  \n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "            \n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.w2v[w] for w in words if w in self.w2v]\n",
    "                   or [np.zeros(self.dimensions)], axis=0) for words in X])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E0Ao18wWde4I"
   },
   "source": [
    "To be able to use in a pipeline, need fit and transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NwerYehyde4J"
   },
   "source": [
    "### Random Forest with Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UEbKJIBkde4J"
   },
   "source": [
    "http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/\n",
    "TF-IDF embedding, focus on important words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, target_class, test_size=0.1, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qVfegx6Ode4K"
   },
   "outputs": [],
   "source": [
    "#Use pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import LinearRegression, SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline =  Pipeline([('Word2Vec Vectorizer', W2vVectorizer(glove)),\n",
    "              ('rf', RandomForestClassifier(n_jobs=3))])\n",
    "\n",
    "parameters = {\n",
    "    'rf__n_estimators':(30,50,80),\n",
    "    'rf__criterion':(\"gini\",\"entropy\"),\n",
    "    'rf__min_samples_split':(4,6,8),\n",
    "    'rf__max_features':(\"auto\",\"sqrt\",\"log2\")\n",
    "    \n",
    "}\n",
    "                \n",
    "if __name__ == \"__main__\":\n",
    "    # multiprocessing requires the fork to happen in a __main__ protected\n",
    "    # block\n",
    "\n",
    "    # find the best parameters for both the feature extraction and the\n",
    "    # classifier\n",
    "    grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)\n",
    "\n",
    "    print(\"Performing grid search...\")\n",
    "    print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "    print(\"parameters:\")\n",
    "    pprint(parameters)\n",
    "    t0 = time()\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    print(\"done in %0.3fs\" % (time() - t0))\n",
    "    print()\n",
    "\n",
    "    print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = Pipeline([('Word2Vec Vectorizer', W2vVectorizer(glove)),\n",
    "              ('rf', RandomForestClassifier(n_jobs=3, criterion='gini',  max_features='log2', min_samples_split=4, n_estimators=150))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZpTsS1EWde4Z"
   },
   "source": [
    "The score is lower than the first Word2Vec model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD Classifier in the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4FHOTQx4de4Z"
   },
   "outputs": [],
   "source": [
    "SGD =  Pipeline([('Word2Vec Vectorizer', W2vVectorizer(glove)),\n",
    "              ('SGD', SGDClassifier(alpha=0.0001, max_iter=80, n_jobs=3))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LErTi1sdde4c"
   },
   "outputs": [],
   "source": [
    "SGD.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i8qyNGZtde4e"
   },
   "outputs": [],
   "source": [
    "SGD.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGD performs worse than RandomForest in this case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MfC_eg3pde4g"
   },
   "source": [
    "## Model 4: Deep Learning Models with Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xPhRNfUHde4g",
    "outputId": "f061e79d-c2bd-4847-f5ff-a12378980948"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Dense, LSTM, Embedding\n",
    "from keras.layers import Dropout, Activation, Bidirectional, GlobalMaxPool1D\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.layers import Flatten\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will map each job description into a real vector domain, a popular technique when working with text called word embedding. This is a technique where words are encoded as real-valued vectors in a high dimensional space, where the similarity between words in terms of meaning translates to closeness in the vector space.\n",
    "\n",
    "Keras provides a convenient way to convert positive integer representations of words into a word embedding by an Embedding layer.\n",
    "\n",
    "We will map each word onto a 64 length real valued vector (embedding size). We will also limit the total number of words that we are interested in modeling to the 1000 most frequent words, and zero out the rest. Finally, the sequence length (number of words) in each review varies, so we will constrain each job description to be 500 words, truncating long reviews and pad the shorter reviews with zero values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MlfCJ3Sede4m"
   },
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer() \n",
    "tokenizer.num_words = 1000  #limit the number of words to keep most common 1000 words\n",
    "tokenizer.fit_on_texts(list(salary_df['combined_text']))\n",
    "list_tokenized_jd = tokenizer.texts_to_sequences(salary_df['combined_text'])\n",
    "\n",
    "X_t = sequence.pad_sequences(list_tokenized_jd, maxlen=400, padding = 'post') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0dKkpHICde4n",
    "outputId": "d58d6c38-af96-4a5b-a538-f554925fa067"
   },
   "outputs": [],
   "source": [
    "len(list_tokenized_jd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mq_ZD2-Ede4j"
   },
   "outputs": [],
   "source": [
    "#Encoding to categorical classes\n",
    "y = to_categorical(target_class.values, num_classes=6, dtype=\"float32\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i8B6kWwTde4q",
    "outputId": "c09f418c-928a-43cc-c939-7aa53053697b"
   },
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n-t9KkH9de4t"
   },
   "source": [
    "#### Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h1Fkq34-de4u"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_t, y, test_size=0.2, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f_WUVyTLde4v",
    "outputId": "da10402f-f598-44e7-c926-1b3a3c489541",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gu7pPgf5de4x",
    "outputId": "b5116b1f-3152-4a9f-d034-c0739f7c6734"
   },
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nwe00hXOde4_",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "embedding_size = 64  #each word onto a 32 length real valued vector\n",
    "model.add(Embedding(input_dim = 1000, output_dim = embedding_size, input_length = 400)) #try 100, 200, single LSTM and single dense 100\n",
    "model.add(LSTM(100))\n",
    "#model.add(LSTM(64))\n",
    "#model.add(GlobalMaxPool1D())\n",
    "#model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "#model.add(Flatten())   #add Flatten layer \n",
    "model.add(Dense(6, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ROzc4IHWde5A",
    "outputId": "aa9d8204-8392-4f42-c7d9-ab1a4cd4b09d"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I8NdwrUTde5D"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZiBp9FQTde5E",
    "outputId": "7c34871d-05c2-4b20-89e1-1525fe42afc6",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=20, batch_size=50, validation_data=[X_test, y_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XDpePK3ude5H"
   },
   "source": [
    "Hyperparameter testing (layer size, learning rate) \n",
    "Grid Search: https://github.com/autonomio/talos\n",
    "https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/\n",
    "Google Collab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zb4F2jC3de5I",
    "outputId": "7b2298e2-4ba2-496a-f6af-114003a8e467"
   },
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "_, train_acc = model.evaluate(X_train, y_train, verbose=0)\n",
    "_, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
    "# plot training history\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.title('Training and test loss at each epoch', fontsize=14)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Neural Network model with simple LSTM layer performed not much better than previous SGD or RF Classification models. We will perform some hyperparameter tuning in the following section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning the Network Model\n",
    "\n",
    "- Use Word2Vec/GloVe embedding:https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
    "\n",
    "- Optimizers\n",
    "- Learning Rate\n",
    "- Batch Size\n",
    "- Regularization: prevent overfitting\n",
    "- Dropout\n",
    "- Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning on a regular embedding layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_t, y, test_size=0.1, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add Convolutional Layer and MaxPooling Layer\n",
    "model = Sequential()\n",
    "\n",
    "embedding_size = 128  #each word onto a 128 length real valued vector\n",
    "model.add(Embedding(input_dim = 1000, output_dim = embedding_size, input_length = 400)) #try 100, 200, single LSTM and single dense 100\n",
    "model.add(Conv1D(filters=32, kernel_size = 3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(LSTM(64))\n",
    "#model.add(LSTM(64))\n",
    "#model.add(GlobalMaxPool1D())\n",
    "#model.add(Dropout(0.5))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "#model.add(Dense(32, activation='relu'))\n",
    "#model.add(Flatten())   #add Flatten layer \n",
    "model.add(Dense(6, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=[X_test, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "_, train_acc = model.evaluate(X_train, y_train, verbose=0)\n",
    "_, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Train Accuracy: %.3f, Test Accuracy: %.3f' % (train_acc, test_acc))\n",
    "\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.title(\"Train vs Test Loss at each epoch\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try fewer input dimension and input words, add Early Stop layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add Early Stopping layer, L1 and L2 regularization\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import regularizers\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "embedding_size = 64  \n",
    "model.add(Embedding(input_dim = 1000, output_dim = embedding_size, input_length = 200)) #try 100, 200, single LSTM and single dense 100\n",
    "#Take out conv1d and maxpooling\n",
    "model.add(Conv1D(filters=32, kernel_size = 3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(LSTM(128, return_sequences=True)) #the last LSTM doesn't need  to return sequence\n",
    "#add LSTM\n",
    "\n",
    "#add dropout here\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())   #add Flatten layer \n",
    "model.add(Dense(6, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', patience = 5, mode='min', verbose=1)\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=30, batch_size=32, validation_data=[X_test, y_test], callbacks=[es])\n",
    "# evaluate the model\n",
    "_, train_acc = model.evaluate(X_train, y_train, verbose=0)\n",
    "_, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Train Accuracy: %.3f, Test Accuracy: %.3f' % (train_acc, test_acc))\n",
    "# plot training history\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.title(\"Train vs Test Loss at each epoch\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 5: Use pre-trained Word2Vec as the embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nw7rYGqdde2e"
   },
   "outputs": [],
   "source": [
    "w2v_model2 = Word2Vec(data,\n",
    "                size=100, #size is the number of dimensions of the N-dimensional space and Word2Vec maps the words into\n",
    "                window=5,\n",
    "                min_count=3, #min frequency appeared in the corpus\n",
    "                workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UZ0XpKJkde2h",
    "outputId": "06eb2c99-3434-4668-d890-e61ea841fbb3"
   },
   "outputs": [],
   "source": [
    "w2v_model2.train(data,total_examples=w2v_model2.corpus_count, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "60ZVS0yRde2o"
   },
   "outputs": [],
   "source": [
    "wv = w2v_model2.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model to be used in deep learning step\n",
    "filename = 'jd_embedding_word2vec2.txt'\n",
    "w2v_model2.wv.save_word2vec_format(filename, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the word embedding as a directory of words to vectors\n",
    "\n",
    "import os\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join('','jd_embedding_word2vec2.txt'), encoding = \"utf-8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:])\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the total_vocabulary in the corpus\n",
    "total_vocabulary = set(word for jd in data for word in jd)\n",
    "\n",
    "max_length=len(total_vocabulary)\n",
    "print('There are {} unique tokens in the dataset.'.format(len(total_vocabulary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the word embedding into tokenized vector. \n",
    "new_tokenizer = text.Tokenizer() \n",
    "new_tokenizer.num_words = 1000  #limit the number of words to keep most common 1000 words\n",
    "new_tokenizer.fit_on_texts(list(salary_df['combined_text']))\n",
    "list_tokenized_jd = new_tokenizer.texts_to_sequences(salary_df['combined_text'])\n",
    "\n",
    "#pad sequences\n",
    "word_index = new_tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "jd_pad = sequence.pad_sequences(list_tokenized_jd, maxlen=500, padding = 'post') \n",
    "print('Shape of the jd tensor:', jd_pad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#map embeddings from word2vec model for each word to new_tokenzier.word_index and create a matrix with word vectors\n",
    "\n",
    "num_words = len(word_index)+1  #1000  + 1\n",
    "embedding_matrix = np.zeros((num_words, 100)) #embedding size = 100\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i > num_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        #words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, GRU\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.initializers import Constant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "shall we limit the input_dim since it's too big? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#define model\n",
    "model = Sequential()\n",
    "\n",
    "embedding_size = 100  #word2vec has a 100 dimension space\n",
    "embedding_layer = Embedding(input_dim = num_words, #input_dim: size of the vocabulary\n",
    "                    output_dim = embedding_size,  #dimension of the dense embedding\n",
    "                    embeddings_initializer = Constant(embedding_matrix),\n",
    "                    input_length = 500, #length of input sequences\n",
    "                    trainable = False)\n",
    "model.add(embedding_layer)\n",
    "#model.add(LSTM(128))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Flatten())   #add Flatten layer \n",
    "model.add(Dense(6, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(jd_pad, y, test_size=0.1, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lower the learning rate since it is training too fast. Limit the input length\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=[X_test, y_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning the model\n",
    "\n",
    "https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 100  #word2vec has a 100 dimension space\n",
    "embedding_layer = Embedding(input_dim = num_words, #input_dim: size of the vocabulary\n",
    "                    output_dim = embedding_size,  #dimension of the dense embedding\n",
    "                    embeddings_initializer = Constant(embedding_matrix),\n",
    "                    input_length = 500, #length of input sequences\n",
    "                    trainable = False)\n",
    "\n",
    "sequence_input  = Input(shape=(500,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "x = MaxPooling1D(3)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dense(16, activation='relu')(x)\n",
    "preds = Dense(6, activation='softmax')(x)\n",
    "\n",
    "opt = Adam(learning_rate=0.001)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec + MLP got a validation accuracy of 0.5258 - still not very good. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "def create_model(learn_rate=0.1):\n",
    "    # create model\n",
    "    embedding_size = 100  #word2vec has a 100 dimension space\n",
    "    embedding_layer = Embedding(input_dim = num_words, #input_dim: size of the vocabulary\n",
    "                    output_dim = embedding_size,  #dimension of the dense embedding\n",
    "                    embeddings_initializer = Constant(embedding_matrix),\n",
    "                    input_length = 500, #length of input sequences\n",
    "                    trainable = False)\n",
    "\n",
    "    sequence_input  = Input(shape=(500,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "    x = Conv1D(128, 10, activation='relu')(embedded_sequences)\n",
    "    x = MaxPooling1D(5)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    preds = Dense(6, activation='softmax')(x)\n",
    "    \n",
    "    opt=Adam(learning_rate=learn_rate)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    \n",
    "    \n",
    "    #Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0, epochs=20, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'learn_rate':[0.001,0.005,0.01,0.05]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Count Vectorizer and TFIDF in Deep Learning Models with Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = salary_df['combined_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "count_vectorizer = CountVectorizer(max_features = 300, max_df = 0.5, min_df = 0.03, ngram_range=(1,3), stop_words='english')\n",
    "word_count_vector = count_vectorizer.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfidf_transformer=TfidfTransformer(smooth_idf=True, use_idf=True)\n",
    "tfidf_transformer.fit(word_count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_idf = pd.DataFrame(tfidf_transformer.idf_, index=count_vectorizer.get_feature_names(), columns=['idf_weights'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_idf.sort_values(by=['idf_weights'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_mat = tfidf_transformer.transform(word_count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(tfidf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Each job description has 300 features, each feature is one word\n",
    "tfidf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding to categorical classes\n",
    "y = to_categorical(target_class.values, num_classes=6, dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train - Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_mat, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(100, input_shape=(300,), activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "#model.add(Flatten())   #add Flatten layer \n",
    "model.add(Dense(6, activation='softmax'))\n",
    "\n",
    "opt = optimizers.Adam(learning_rate=0.001)\n",
    "#Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt,\n",
    "          metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', patience = 10, mode='min', verbose=1)\n",
    "\n",
    "history = model.fit(X_train, y_train,  validation_data=(X_test, y_test), epochs=40, batch_size=64, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "_, train_acc = model.evaluate(X_train, y_train, verbose=0)\n",
    "_, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
    "# plot training history\n",
    "plt.plot(history.history['accuracy'], label='train')\n",
    "plt.plot(history.history['val_accuracy'], label='test')\n",
    "plt.title('Training and test Accuracy at each epoch', fontsize=14)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train loss is 0.904, Test loss is 0.518"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "def create_model(neurons=1):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, input_shape=(300,), activation='relu'))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    #model.add(Flatten())   #add Flatten layer \n",
    "    model.add(Dense(6, activation='softmax'))\n",
    "    \n",
    "    #Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0, epochs=20, batch_size=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Define the grid search parameters\n",
    "#learning_rate = [0.001,0.0001]\n",
    "\n",
    "param_grid = {\n",
    "    'neurons':[32,64,128,256]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(tfidf_mat, y)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results from various grid search show that optimizer='Adam\",  epochs=20, activation='relu' produce a better result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 6: Mixed Input Model with City, Rate Type and Cost of Living Indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Functional API:  https://machinelearningmastery.com/keras-functional-api-deep-learning/\n",
    "\n",
    "Multi-input full example: https://www.pyimagesearch.com/2019/02/04/keras-multiple-inputs-and-mixed-data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore continuous variables and categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "salary_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "cont_features = ['Cost of Living Index','Rent Index','Cost of Living Plus Rent Index','Local Purchasing Power Index']\n",
    "X_cont_df = salary_df.loc[:, cont_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cont_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cont_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Look at the correlation among continous variables\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "plt.figure(figsize=(8,6))\n",
    "ax = sns.heatmap(X_cont_df.corr())\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like Cost of Living Index largely correlates with Rent Index and negatively correlates with Local Purchasing Power Index. We will keep Cost of Living Plus Rent Index and Local Purchasing Power Index in this model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep Cost of Living Plus Rent Index, Local Purchasing Power and Fill missing values with the mean\n",
    "\n",
    "cont_imp = SimpleImputer(strategy='mean')\n",
    "cont_imp.fit(X_cont_df[['Cost of Living Plus Rent Index','Local Purchasing Power Index']])\n",
    "X_cont = cont_imp.transform(X_cont_df[['Cost of Living Plus Rent Index','Local Purchasing Power Index']])\n",
    "\n",
    "#Standardized inputs\n",
    "\n",
    "sc=StandardScaler()\n",
    "sc.fit(X_cont)\n",
    "X_cont_scaled = sc.transform(X_cont)\n",
    "\n",
    "#Continous Features Dataframe\n",
    "cont_scaled_df = pd.DataFrame(X_cont_scaled, columns = ['Cost of Living Plus Rent Index','Local Purchasing Power Index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try unscaled Index\n",
    "con_unscaled_df = pd.DataFrame(X_cont_df, columns = ['Cost of Living Plus Rent Index','Local Purchasing Power Index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_unscaled_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#salary_df['City_Bins'] = salary_df['City_Bins'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Categorical Features\n",
    "\n",
    "cat_features=['Rate_by','City_Bins']\n",
    "X_cat = salary_df.loc[:, cat_features]\n",
    "\n",
    "# Replace NaNs with 'missing'\n",
    "cat_imp = SimpleImputer(strategy='constant', fill_value='missing')\n",
    "cat_imp.fit(X_cat)\n",
    "X_cat = cat_imp.transform(X_cat)\n",
    "\n",
    "#Encode Categorical Features\n",
    "ohe = OneHotEncoder(handle_unknown='ignore')\n",
    "ohe.fit(X_cat)\n",
    "X_cat_ohe = ohe.transform(X_cat)\n",
    "\n",
    "#Create categorical features dataframe\n",
    "cat_df = pd.DataFrame(X_cat_ohe.todense(), columns=ohe.get_feature_names(input_features=cat_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cat_df['City_Bins']= salary_df['City_Bins']\n",
    "#drop Rate_by_Daily and City_Bins_missing since too many zeros\n",
    "cat_df_clean = cat_df.drop(['Rate_by_Daily','City_Bins_missing'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine contious and categorical variables into one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Other_inputs = pd.concat([cont_scaled_df, cat_df], axis=1)\n",
    "Other_inputs = pd.concat([cont_scaled_df, cat_df_clean], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Other_inputs.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text inputs - embedding layer\n",
    "tokenizer = text.Tokenizer() \n",
    "tokenizer.num_words = 1000  #limit the number of words to keep most common 1000 words\n",
    "tokenizer.fit_on_texts(list(salary_df['combined_text']))\n",
    "list_tokenized_jd = tokenizer.texts_to_sequences(salary_df['combined_text'])\n",
    "\n",
    "X_t = sequence.pad_sequences(list_tokenized_jd, maxlen=500, padding = 'post') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(target_class.values, num_classes=6, dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Test split on text inputs use Keras embedding layer\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_t, y, test_size=0.1, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Test split on other inputs\n",
    "\n",
    "X2_train, X2_test, y_train, y_test = train_test_split(Other_inputs, y, test_size=0.1, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "target_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras.layers import concatenate\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UpLPFthfde5J"
   },
   "outputs": [],
   "source": [
    "# define two sets of inputs\n",
    "inputA = Input(shape=(8,)) #non-words input\n",
    "inputB = Input(shape=(500,))  #word vectors \n",
    "\n",
    "# the first branch operates on the first input\n",
    "#hiddenA1 = Dense(16, activation=\"relu\")(inputA)\n",
    "#hiddenA2 = Dense(16, activation=\"relu\")(hiddenA1)\n",
    "outputA = Dense(4, activation=\"relu\")(inputA)\n",
    "modelA = Model(inputs=inputA, outputs=outputA)\n",
    "\n",
    "# the second branch opreates on the second input\n",
    "embedB1 =  Embedding(input_dim = 1000, output_dim = 32, input_length = 500)(inputB) #each word onto a 32 length real valued vector\n",
    "convB1 = Conv1D(filters=32, kernel_size = 3, padding='same', activation='relu')(embedB1)\n",
    "poolB1 = MaxPooling1D(pool_size=2)(convB1)\n",
    "LSTMB1 = LSTM(128, return_sequences=True)(poolB1)\n",
    "hiddenB1 = Dense(100, activation='relu')(LSTMB1)\n",
    "dropB1 = Dropout(0.5)(hiddenB1)\n",
    "flat = Flatten()(dropB1)\n",
    "outputB = Dense(128, activation='relu')(flat)\n",
    "modelB = Model(inputs=inputB, outputs=outputB)\n",
    "\n",
    "# combine the output of the two branches\n",
    "combined = concatenate([modelA.output, modelB.output])\n",
    "# apply a FC layer and then a regression prediction on the\n",
    "# combined outputs\n",
    "z = Dense(16, activation=\"relu\")(combined)\n",
    "z = Dense(6, activation=\"softmax\")(z)\n",
    "# our model will accept the inputs of the two branches and\n",
    "# then output a single value\n",
    "model = Model(inputs=[modelA.input, modelB.input], outputs=z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ehW6a2HGde5O"
   },
   "outputs": [],
   "source": [
    "\n",
    "opt = Adam(lr=1e-3, decay=1e-3/200)\n",
    "model.compile(loss = 'categorical_crossentropy', metrics=['accuracy'], optimizer=opt)\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', patience = 10, mode='min', verbose=1)\n",
    "\n",
    "history = model.fit([X2_train,X_train], y_train, validation_data = ([X2_test, X_test], y_test), epochs=40, batch_size=128, callbacks=[es])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Word2Vec Embedding in Mixed Input Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test using pad sequences from Word2Vec model\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(jd_pad, y, test_size=0.1, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define two sets of inputs\n",
    "inputA = Input(shape=(8,)) #non-words input\n",
    "inputB = Input(shape=(500,))  #word vectors \n",
    "\n",
    "# the first branch operates on the first input\n",
    "#hiddenA1 = Dense(16, activation=\"relu\")(inputA)\n",
    "#hiddenA2 = Dense(16, activation=\"relu\")(hiddenA1)\n",
    "outputA = Dense(4, activation=\"relu\")(inputA)\n",
    "modelA = Model(inputs=inputA, outputs=outputA)\n",
    "\n",
    "# the second branch opreates on the second input\n",
    "\n",
    "#Word2Vec Embedding layer\n",
    "embedding_size = 100  #word2vec has a 100 dimension space\n",
    "embedding_layer = Embedding(input_dim = num_words, #input_dim: size of the vocabulary\n",
    "                    output_dim = embedding_size,  #dimension of the dense embedding\n",
    "                    embeddings_initializer = Constant(embedding_matrix),\n",
    "                    input_length = 500, #length of input sequences\n",
    "                    trainable = False)\n",
    "\n",
    "\n",
    "embedded_sequences = embedding_layer(inputB)\n",
    "#embedB1 =  Embedding(input_dim = 1000, output_dim = 32, input_length = 500)(inputB) #each word onto a 32 length real valued vector\n",
    "convB1 = Conv1D(filters=32, kernel_size = 3, padding='same', activation='relu')(embedded_sequences)\n",
    "poolB1 = MaxPooling1D(pool_size=2)(convB1)\n",
    "LSTMB1 = LSTM(128, return_sequences=True)(poolB1)\n",
    "dropB1 = Dropout(0.5)(poolB1)\n",
    "hiddenB1 = Dense(100, activation='relu')(dropB1)\n",
    "dropB2 = Dropout(0.5)(hiddenB1)\n",
    "flat = Flatten()(dropB2)\n",
    "outputB = Dense(128, activation='relu')(flat)\n",
    "modelB = Model(inputs=inputB, outputs=outputB)\n",
    "\n",
    "# combine the output of the two branches\n",
    "combined = concatenate([modelA.output, modelB.output])\n",
    "# apply a FC layer and then a regression prediction on the\n",
    "# combined outputs\n",
    "z = Dense(64, activation=\"relu\")(combined)\n",
    "z = Dense(32, activation=\"relu\")(z)\n",
    "z = Dense(6, activation=\"softmax\")(z)\n",
    "# our model will accept the inputs of the two branches and\n",
    "# then output a single value\n",
    "model = Model(inputs=[modelA.input, modelB.input], outputs=z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = RMSprop(lr=1e-4)\n",
    "model.compile(loss = 'categorical_crossentropy', metrics=['accuracy'], optimizer=opt)\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', patience = 10, mode='min', verbose=1)\n",
    "\n",
    "history = model.fit([X2_train,X_train], y_train, validation_data = ([X2_test, X_test], y_test), epochs=40, batch_size=128, callbacks=[es])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec was not able to improve the performance =  ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 7: Use Job Title along with City, Cost of Living Index in Classification Models \n",
    "\n",
    "Model 1-5 uses job description and job title as the combined text in the text classification model. However, due to the large number of words in one job description, using the whole text may not be the best way to extract information as we can see many unrelevant words show up even with TF-IDF. Now we are going to use Title as a separate set of features to predict salary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop NA values\n",
    "salary_df_new = salary_df.dropna(axis = 0, subset = ['City_Bins'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_df_new.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_df_new['Title'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_df_new = salary_df_new.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_df_new = salary_df_new.drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_title = salary_df_new['Title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use count vectorizer to examine title\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(stop_words='english', ngram_range=(1,3), max_df=0.3, min_df=0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitting count vectorizer to job title\n",
    "\n",
    "cv.fit(job_title)\n",
    "\n",
    "title_words = pd.DataFrame(cv.transform(job_title).todense(), columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_words.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One-hot encoding of categorical feature City_Bins\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "labelencoder = LabelEncoder()\n",
    "\n",
    "salary_df_new['City_cat']=labelencoder.fit_transform(salary_df_new['City_Bins'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_df_new['City_cat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construct model dataset\n",
    "X = pd.concat([title_words, salary_df_new['City_cat'], salary_df_new['Cost of Living Plus Rent Index']], axis=1)\n",
    "y = salary_df_new['Salary_Bins'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Add City Bins and Local Purchase Power Index\n",
    "word_observe = pd.concat([title_words, y], axis=1)\n",
    "\n",
    "word_observe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table=pd.pivot_table(word_observe, index=['Salary_Bins'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the distribution above, we can tell some words appear more for higher salary bin than others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'Word' digital mostly appear in lower bins\n",
    "table['digital'].plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'Word' director mostly appear in higher bins\n",
    "table['director'].plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word 'soft engineer' mostly appear in lower bins\n",
    "table['software engineer'].plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#How to use the frequency as features in the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looks \n",
    "for i in range(0,6):\n",
    "    word_count = word_observe[word_observe['Salary_Bins']==i].sum(axis=0)\n",
    "    print(i, \"Bin most common words:\")\n",
    "    cw = word_count.sort_values(ascending=False).head(20)\n",
    "    print(cw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest as base model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inititate RF classifier\n",
    "rf = RandomForestClassifier()\n",
    "parameters = {\n",
    "    'criterion':(\"gini\",\"entropy\"),\n",
    "    'min_samples_split':(2,6,8),\n",
    "    'max_features':(\"auto\",\"sqrt\",\"log2\"),\n",
    "    'max_samples':(None, 0.5,0.8,1) \n",
    "    \n",
    "}\n",
    "\n",
    "clf = GridSearchCV(rf, parameters)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.best_estimator_.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check feature importance of the best performing model\n",
    "\n",
    "rf = RandomForestClassifier(criterion='gini', max_samples=0.8)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "importances = rf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA in the pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "rf = RandomForestClassifier(criterion='gini', max_samples=0.8)\n",
    "\n",
    "pipe = Pipeline(steps=[('pca', pca), ('rf', rf)])\n",
    "\n",
    "param  = {\n",
    "    'pca__n_components':[3,5,10,15,20,25],\n",
    "    'rf__criterion':(\"gini\",\"entropy\"),\n",
    "    'rf__min_samples_split':(2,6,8),\n",
    "}\n",
    "\n",
    "search = GridSearchCV(pipe, param, n_jobs=-1)\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameter (CV score = %0.3f):\" % search.best_score_)\n",
    "print(search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "XGB = GradientBoostingClassifier(random_state=4)\n",
    "\n",
    "pipe = Pipeline(steps=[('pca', pca), ('xgb', XGB)])\n",
    "\n",
    "param  = {\n",
    "    'pca__n_components':[3,5,10,15,20,25],\n",
    "    'xgb__loss':(\"deviance\",\"exponential\"),\n",
    "    'xgb__learning_rate':(0.1, 0.01, 0.001),\n",
    "    'xgb__subsample':(0.3,0.5,1.0),\n",
    "}\n",
    "\n",
    "search = GridSearchCV(pipe, param, n_jobs=-1)\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameter (CV score = %0.3f):\" % search.best_score_)\n",
    "print(search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among 7 models we built, Model 2 performs the best with a test accuracy of 0.71. We will use this model for deployment. \n",
    "\n",
    "- Model 1: Word2Vec Embedding + Classification Models (Random Forest/SGD) \n",
    "- Model 2: Count Vectorizer + TF-IDF Transformation + Classification Models (Random Forest/SGD/SVM)\n",
    "- Model 3: Pre-trained GloVe Embedding + Classification Models (Random Forest/SGD)\n",
    "- Model 4: Word Embeddings + Simple Neural Network Model\n",
    "- Model 5: Pre-trained GloVe Embedding + Deep Neural Network Model\n",
    "- Model 6: Mixed Input Network Model with City, Rate Type, and Cost of Living Indexes\n",
    "- Model 7: Just use Job Title, City, Cost of Living Indexes + Classification\n",
    "\n",
    "For next steps:\n",
    "1. Job salary range can change from time to time depends on economic situation and job market. Therefore the model needs to be constantly trained with refreshed data to produce up to date results. Most of the positions were pulled in late May 2020 when COVID19 impacted North America so the salary range during this period of time may not reflect the \"normal\". \n",
    "\n",
    "2. We scrapped the full job description from Indeed as there is no standard way to parse out company detail, skills requirements and benefits so there are noises within the job description text to extract features. \n",
    "\n",
    "3. The size of the salary labeled data is small. In the future, scrape more data with salary to enhance the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Salary_Modeling_5.31.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "265.969px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": "20",
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "208px",
    "left": "797px",
    "right": "20px",
    "top": "-57px",
    "width": "282px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
